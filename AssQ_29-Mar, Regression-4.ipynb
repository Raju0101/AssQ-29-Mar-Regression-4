{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2172d05c-4302-4cb4-9677-927e5c5d4e36",
   "metadata": {},
   "source": [
    "# AssQ 29-March, Regression-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7ba75-d124-4480-a712-52d408833fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16b4a5-cc04-4ab9-92ee-2fe30421e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. \n",
    "This model uses shrinkage. \n",
    "Shrinkage is where data values are shrunk towards a central point as the mean.\n",
    "\n",
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing\n",
    "a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "The lasso regression allows you to shrink or regularize these coefficients to avoid overfitting and make them work\n",
    "better on different datasets. This type of regression is used when the dataset shows high multicollinearity or\n",
    "when you want to automate variable elimination and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7aae4-8b49-4ad1-87cf-a653846f4893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cf7e1-5287-4158-a3eb-29a390dc8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57cc9d-b774-4602-af42-2b266e19bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trying to minimize the cost function, Lasso regression will automatically select those features that are useful,\n",
    "discarding the useless or redundant features.\n",
    "In Lasso regression, discarding a feature will make its coefficient equal to 0.\n",
    "\n",
    "The main advantage of a LASSO regression model is that it has the ability to set the coefficients for\n",
    "features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide\n",
    "which features should and should not be included on its own.\n",
    "\n",
    "Another advantage of LASSO versus many other subset selection methods is that it favors subsets of features that \n",
    "have less collinearity. Since prediction metrics often are not harmed by collinearity, \n",
    "subset selection techniques that rely on prediction metrics will often fail to exclude highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c67c83-6567-44bd-88e6-bc721bea3fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d07e72-634b-4463-a75a-5637eee44e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc7e1b-4f34-4e7f-aaeb-f1c86d9a0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso estimates of the coefficients (Tibshirani, 1996) achieve min β ( Y − X β ) ′ ( Y − X β ) + λ ∑ j = 1 p | β j | , \n",
    "so that the L2 penalty of ridge regression ∑ j = 1 p β j 2 is replaced by an L1 penalty, ∑ j = 1 p | β j | .\n",
    "\n",
    "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of\n",
    "the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; \n",
    "Some coefficients can become zero and eliminated from the model.\n",
    "\n",
    "The lasso penalty will force some of the coefficients quickly to zero. This means that variables are removed\n",
    "from the model, hence the sparsity. Ridge regression will more or less compress the coefficients to become smaller. \n",
    "This does not necessarily result in 0 coefficients and the removal of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68207aa-0766-48e4-a2d5-3092a453e5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b4099-c68d-4e4b-83a9-9877b97cfa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the \n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4aa0b-ff50-4ccc-9761-05dbeeb3137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term \n",
    "in ridge regression and lasso regression. It is basically the amount of shrinkage,\n",
    "where data values are shrunk towards a central point, like the mean.\n",
    "\n",
    "\n",
    "Since it takes absolute values, hence, it can shrink the slope to 0. Some of the features in the dataset\n",
    "are completely neglected for model evaluation. It can help us to reduce the overfitting in the model \n",
    "as well as the feature selection. It is also called as L1 regularization.\n",
    "\n",
    "One obvious advantage of lasso regression over ridge regression, is that it produces simpler and more \n",
    "interpretable models that incorporate only a reduced set of the predictors. However, \n",
    "neither ridge regression nor the lasso will universally dominate the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24627a77-35d0-4ecc-96be-2eb66592fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b062679-1f5a-4a51-ab40-af5cfa9bb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157901ab-a3e6-4f74-81d3-7ad080043952",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in\n",
    "linear regression models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression\n",
    "model and thereby selecting the number of basis functions effectively.\n",
    "\n",
    "Lasso is a modification of linear regression, where the model is penalized for the sum of absolute values of the weights.\n",
    "\n",
    "If we can linearize the model, then yes but for an approximate solution in \n",
    "the LS sense since what is measured is y and not any of its possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71c0e9-d9eb-42a5-ade4-145803d21cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5344afa-544a-4dc9-9773-c08eafd88664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af221a-e841-46c7-8dc3-b21dbc560699",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. \n",
    "However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. \n",
    "Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "Lasso will eliminate many features, and reduce overfitting in your linear model.\n",
    "Ridge will reduce the impact of features that are not important in predicting your y values.\n",
    "Elastic Net combines feature elimination from Lasso and feature coefficient reduction\n",
    "from the Ridge model to improve your model's predictions.\n",
    "\n",
    "The main difference between Lasso and Ridge is the penalty term they use.\n",
    "Ridge uses L2 penalty term which limits the size of the coefficient vector.\n",
    "Lasso uses L1 penalty which imposes sparsity among the coefficients and thus,\n",
    "makes the fitted model more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e7829-7d24-47ef-b7d4-9d53f61a47a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b3df8-df78-43b4-bd29-107a14a1dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce435b6d-c98c-4f6a-a6d5-c79936722ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator\n",
    "(LASSO) regression, solves the same constrained optimization problem as ridge regression,\n",
    "but uses the L1 norm rather than the L2 norm as a measure of complexity.\n",
    "\n",
    "LASSO and Ridge regression are advanced forms of regression analysis that can handle multicollinearity. \n",
    "If you know how to perform linear least squares regression, \n",
    "you'll be able to handle these analyses with just a little additional study.\n",
    "\n",
    "It shrinks coefficients to zero (compare to Ridge which adds “squared magnitude” of coefficient \n",
    "as penalty term to the loss function). If group of predictors are highly correlated,\n",
    "lasso picks only one of them and shrinks the others to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4856b-c00a-4a46-91f9-481500834fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dff942-a909-423b-abea-a1481fedfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f372a-7eb9-4d53-9e6d-909f21a7130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "For lasso regression, the alpha value is 1. The output is the best cross-validated lambda,\n",
    "which comes out to be 0.001. Once we have the optimal lambda value.\n",
    "\n",
    "Lasso regression can be used for automatic feature selection, as the geometry of its constrained region\n",
    "allows coefficient values to inert to zero. An alpha value of zero in either ridge or lasso model will \n",
    "have results similar to the regression model.\n",
    "The larger the alpha value, the more aggressive the penalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
